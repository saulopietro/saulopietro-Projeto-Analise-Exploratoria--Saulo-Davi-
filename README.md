üìä An√°lise de Dados ‚Äì Projeto Olist E-commerce
üë• Integrantes da Equipe

Saulo Pietro

Davi Silva

üîó Base de Dados Utilizada

Fonte: Olist Brazilian E-commerce Dataset
Link: https://www.kaggle.com/datasets/olistbr/brazilian-ecommerce

üéØ Objetivo do Projeto

Este trabalho busca investigar padr√µes presentes no e-commerce brasileiro, identificando fatores que podem influenciar:

experi√™ncia e satisfa√ß√£o do cliente,

custo de frete,

caracter√≠sticas f√≠sicas dos produtos,

poss√≠veis inconsist√™ncias e limita√ß√µes da base de dados.

Al√©m de entender o comportamento geral do marketplace, o estudo fornece insumos √∫teis para an√°lises futuras, incluindo modelos preditivos e valida√ß√µes estat√≠sticas.

üßπ Processo de Tratamento e Prepara√ß√£o dos Dados
1. Importa√ß√£o e explora√ß√£o inicial

Foram avaliados tipos de vari√°veis, presen√ßa de valores nulos, registros duplicados e poss√≠veis anomalias. Isso permitiu mapear rapidamente os pontos cr√≠ticos da base.

2. Remo√ß√£o de duplicidades

Registros repetidos foram identificados por df.duplicated() e eliminados para evitar distor√ß√µes nas m√©tricas e agrega√ß√µes.

3. Valores ausentes (missing data)

Valores nulos foram tratados conforme o tipo da coluna:

Pre√ßos e fretes receberam imputa√ß√£o baseada em mediana.

Datas inv√°lidas foram convertidas para NaT.

Colunas categ√≥ricas tiveram preenchimento com r√≥tulos neutros como "unknown".

Esse procedimento evitou a perda desnecess√°ria de dados relevantes.

4. Outliers

Foram detectados outliers usando medidas robustas (como IQR).
Em vez de simplesmente excluir linhas extremas, optou-se por:

limitar valores muito discrepantes,

corrigir datas imposs√≠veis,

e manter atrasos leg√≠timos, j√° que refletem comportamento real da opera√ß√£o log√≠stica.

5. Normaliza√ß√£o e padroniza√ß√£o

Duas t√©cnicas foram aplicadas conforme a necessidade:

MinMaxScaler ‚Üí escala 0‚Äì1 para compara√ß√£o direta entre atributos.

StandardScaler (Z-score) ‚Üí padroniza√ß√£o para an√°lises que exigem distribui√ß√£o mais sim√©trica.

Ambos os scalers tamb√©m foram aplicados √†s vari√°veis derivadas.

6. Feature Engineering

Novos atributos foram criados para enriquecer a an√°lise:

volume do produto (length √ó height √ó width),

densidade aproximada,

rela√ß√µes peso/frete e pre√ßo/peso,

indicadores log√≠sticos calculados por pedido.

Esses atributos ajudaram a explicar melhor varia√ß√µes em frete e atraso.

7. Correla√ß√£o

Foi elaborado um heatmap com todas as vari√°veis num√©ricas (originais e derivadas) para identificar rela√ß√µes estruturais. Isso permitiu destacar padr√µes importantes, como o impacto do tamanho/peso sobre o frete.

üßó Principais Dificuldades Encontradas

Volume elevado de outliers
Grande parte dos produtos apresentava valores extremos em pre√ßo, frete ou dimens√µes.

Escalas muito diferentes
Unidades distintas (cm, gramas, reais, dias) exigiram normaliza√ß√£o cuidadosa para permitir compara√ß√µes coerentes.

Dados incompletos ou inconsistentes
Campos vazios e datas inv√°lidas afetavam an√°lises temporais e precisaram de tratamento dedicado.

Engenharia de atributos sem redund√¢ncia
Criar novas vari√°veis realmente informativas exigiu testes sucessivos para evitar sobreposi√ß√£o de informa√ß√£o.

üí° Principais Conclus√µes

A base apresenta outliers significativos, que precisaram de tratamento robusto para evitar distor√ß√µes.

Caracter√≠sticas f√≠sicas dos produtos (peso, volume e dimens√µes) mostram rela√ß√£o clara com o valor do frete, evidenciando o papel da log√≠stica no custo final.

Ap√≥s normaliza√ß√£o, surgiram padr√µes que n√£o eram vis√≠veis no estado bruto dos dados.

A engenharia de atributos complementou a an√°lise ao revelar como volume, densidade e rela√ß√µes peso/frete influenciam vari√°veis financeiras.

A etapa de limpeza alterou de forma importante a distribui√ß√£o das vari√°veis, reduzindo assimetria e tornando o conjunto mais confi√°vel para an√°lises posteriores.
